{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e439802",
   "metadata": {},
   "source": [
    "# simulate stacking experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2850c2e-1883-4126-8140-901d5898eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from simulation_experiment import synexp\n",
    "from visualize_sim import sim_plots\n",
    "import pandas as pd\n",
    "\n",
    "n_runs = 50 # how many simulation experiments\n",
    "run_sim = True # run experiment?\n",
    "plot_sim = True # plot results?\n",
    "\n",
    "type_sim = 1 # what parameter to vary?\n",
    "# 1 - Vary the dimensionality of X1 with respect to other feature spaces\n",
    "# 2 - Vary the weight of X1 with respect to other feature spaces\n",
    "# 3 - Vary the noise level\n",
    "# 4 - Vary the number of samples\n",
    "# 5 - Vary feature space correlation\n",
    "\n",
    "# Note, the experiments below are from the paper, you can change the parameters as you want\n",
    "# It might be more feasible to make a script and call synexp non-interactively, especially for large experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8faefb0-2b21-46b2-9d2b-6b03b03b5251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number 1\n",
      "data sampled\n",
      "time for stacking: 5.576988935470581\n",
      "time for iteration: 16.36051297187805\n",
      "data sampled\n",
      "time for stacking: 5.68248987197876\n",
      "time for iteration: 17.94617486000061\n",
      "data sampled\n",
      "time for stacking: 10.574278831481934\n",
      "time for iteration: 24.7062509059906\n",
      "data sampled\n",
      "time for stacking: 7.50465989112854\n",
      "time for iteration: 21.93230676651001\n",
      "data sampled\n",
      "time for stacking: 8.925495147705078\n",
      "time for iteration: 29.840813159942627\n",
      "first iteration time: 111, total 5556\n",
      "iteration number 2\n",
      "data sampled\n",
      "time for stacking: 13.184641122817993\n",
      "time for iteration: 35.82826232910156\n",
      "data sampled\n",
      "time for stacking: 15.410903215408325\n",
      "time for iteration: 36.77866816520691\n",
      "data sampled\n",
      "time for stacking: 12.556435108184814\n",
      "time for iteration: 30.479616165161133\n",
      "data sampled\n",
      "time for stacking: 12.968762159347534\n",
      "time for iteration: 31.550554275512695\n",
      "data sampled\n",
      "time for stacking: 9.648847818374634\n",
      "time for iteration: 26.950864791870117\n",
      "iteration number 3\n",
      "data sampled\n",
      "time for stacking: 9.796719074249268\n",
      "time for iteration: 26.401182889938354\n",
      "data sampled\n",
      "time for stacking: 8.374092102050781\n",
      "time for iteration: 23.136788845062256\n",
      "data sampled\n",
      "time for stacking: 8.078734874725342\n",
      "time for iteration: 30.22462296485901\n",
      "data sampled\n",
      "time for stacking: 13.18555498123169\n",
      "time for iteration: 34.938925981521606\n",
      "data sampled\n",
      "time for stacking: 13.350465059280396\n",
      "time for iteration: 33.98402714729309\n",
      "iteration number 4\n",
      "data sampled\n",
      "time for stacking: 11.148561239242554\n",
      "time for iteration: 29.419289112091064\n",
      "data sampled\n",
      "time for stacking: 10.736790180206299\n",
      "time for iteration: 30.648571968078613\n",
      "data sampled\n",
      "time for stacking: 12.025328159332275\n",
      "time for iteration: 28.129180908203125\n",
      "data sampled\n",
      "time for stacking: 9.363476276397705\n",
      "time for iteration: 22.97734808921814\n",
      "data sampled\n",
      "time for stacking: 7.306324005126953\n",
      "time for iteration: 20.82240104675293\n",
      "iteration number 5\n",
      "data sampled\n",
      "time for stacking: 7.735720157623291\n",
      "time for iteration: 24.28347420692444\n",
      "data sampled\n",
      "time for stacking: 8.583776950836182\n",
      "time for iteration: 23.722854137420654\n",
      "data sampled\n",
      "time for stacking: 8.153543710708618\n",
      "time for iteration: 24.09500002861023\n",
      "data sampled\n",
      "time for stacking: 12.154590129852295\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "type_sim = 5 # what parameter to vary?\n",
    "\n",
    "\n",
    "version = 'v5'\n",
    "\n",
    "# ds_settings - dimentionality of each feature space\n",
    "# alpha - stacking weights for simulated data\n",
    "# n - number of samples \n",
    "# sigma - noise evel\n",
    "\n",
    "if type_sim ==1: # Vary the dimensionality of X1 with respect to other feature spaces\n",
    "\n",
    "    ds_settings = [[2,10,10,10],[5,10,10,10],[10,10,10,10],[20,10,10,10],[40,10,10,10]]\n",
    "    n = 100\n",
    "    sigma = 0.2\n",
    "    correl = 0.2\n",
    "    alpha = [0.5,0.2,0.2,0.1] \n",
    "\n",
    "    print('setup')\n",
    "\n",
    "    if run_sim:\n",
    "        print('running')\n",
    "        R_d3  = synexp(runs = n_runs,sim_type = 'Feat_Dim_ratio',samples_settings=n,ds_settings=ds_settings,y_dim=2,\n",
    "                          correl = correl, alpha_settings = alpha,scale = 0.5,y_noise_settings=sigma)\n",
    "        \n",
    "        R_d3.to_pickle('sweep_d_cluster_{}.npy'.format(version))\n",
    "    \n",
    "    if plot_sim:\n",
    "        R_d3 = pd.read_pickle('sweep_d_cluster_{}.npy'.format(version))\n",
    "        var = dict(sigma = sigma, d_sum = sum(ds_settings[0][1:]), n = n,alphas = alpha)\n",
    "        ratio = [d[0] for d in ds_settings]\n",
    "        sim_plots(R_d3,'Feat_Dim_ratio', ratio, filename = 'sweep_d',var_dict = var,ylim0=[-0.1,1],ylim1=[-.2,0.6],\n",
    "                 ylim2=[0,0.5])\n",
    "\n",
    "elif type_sim ==2: # Vary the weight of X1 with respect to other feature spaces\n",
    "\n",
    "    ds = [10,10,10,10]\n",
    "    n = 100\n",
    "    correl = 0.2\n",
    "    sigma = 0.2\n",
    "    alpha =  [[0.1,0.2,0.5,0.2],[0.3,0.2,0.3,0.2],[0.5,0.2,0.2,0.1],[0.7,0.1,0,0.2],[0.9,0.1,0.,0.]] \n",
    "    for a in alpha:\n",
    "        assert np.round(sum(a),2)==1\n",
    "\n",
    "    if run_sim:\n",
    "        R_C3  = synexp(runs = n_runs,sim_type = 'Cond',samples_settings=n,ds_settings=ds,y_dim=2,\n",
    "                          correl = correl, alpha_settings = alpha,scale = 0.5,y_noise_settings=sigma)\n",
    "        \n",
    "        R_C3.to_pickle('sweep_c_cluster_{}.npy'.format(version))\n",
    "    \n",
    "    if plot_sim:\n",
    "        R_C3 = pd.read_pickle('sweep_c_cluster_{}.npy'.format(version))\n",
    "        var = dict(sigma = sigma, ds = ds, n = n,alphas = alpha)\n",
    "        \n",
    "        ratio = [a[0] for a in alpha]\n",
    "        sim_plots(R_C3,'Cond', ratio, filename = 'sweep_alpha',var_dict = var,ylim0=[-0.1,1],ylim1=[-.2,0.6],\n",
    "                 ylim2=[0,0.5])\n",
    "\n",
    "\n",
    "elif type_sim ==3: # Vary the noise level\n",
    "\n",
    "    ds = [10,10,10,10]\n",
    "    n = 100\n",
    "    sigma = [0,0.2,0.5,1,1.5]\n",
    "    alpha = [0.5,0.2,0.2,0.1]\n",
    "    correl = 0.2\n",
    "    assert np.round(sum(alpha),2)==1\n",
    "\n",
    "    if run_sim:\n",
    "        R_sigma3  =synexp(runs = n_runs,sim_type = 'noise',samples_settings=n,ds_settings=ds,y_dim=2,\n",
    "                          correl = correl, alpha_settings = alpha,scale = 0.5,y_noise_settings=sigma)\n",
    "        \n",
    "        R_sigma3.to_pickle('sweep_sigma_cluster_{}.npy'.format(version))\n",
    "    \n",
    "    if plot_sim:\n",
    "        R_sigma3 = pd.read_pickle('sweep_sigma_cluster_{}.npy'.format(version))\n",
    "        var = dict( ds = ds, alphas = alpha,n = n)\n",
    "        sim_plots(R_sigma3,'noise', sigma, filename = 'sweep_sigma',var_dict = var,ylim0=[-0.1,1],ylim1=[-.2,0.6],\n",
    "                 ylim2=[0,0.5])\n",
    "\n",
    "elif type_sim ==4: # Vary the number of samples\n",
    "    \n",
    "    # ds = [10,10,10,10]\n",
    "    ds = [100,100,100,100]\n",
    "    # n = [30,40,60,100,200]\n",
    "    n = [100,200,400, 800]\n",
    "    sigma = 0.2\n",
    "    alpha = [0.5,0.2,0.2,0.1]\n",
    "    correl = 0.2\n",
    "    assert np.round(sum(alpha),2)==1\n",
    "\n",
    "    if run_sim:\n",
    "        R_n3  = synexp(runs = n_runs,sim_type = 'Sample_Dim_ratio',samples_settings=n,ds_settings=ds,y_dim=2,\n",
    "                          correl = correl, alpha_settings = alpha,scale = 0.5,y_noise_settings=sigma)\n",
    "        \n",
    "        R_n3.to_pickle('sweep_n_cluster_{}.npy'.format(version))\n",
    "    \n",
    "    if plot_sim:\n",
    "        R_n3 = pd.read_pickle('sweep_n_cluster_{}.npy'.format(version))\n",
    "    \n",
    "        var = dict(sigma = sigma, ds = ds, alpha = alpha)\n",
    "        sim_plots(R_n3,'Sample_Dim_ratio', n, filename = 'sweep_n',var_dict = var,ylim0=[-0.1,1],ylim1=[-.2,0.6],\n",
    "                 ylim2=[0,0.5])\n",
    "\n",
    " \n",
    "elif type_sim ==5: # Vary the correlation\n",
    "    \n",
    "    # ds = [10,10,10,10] \n",
    "    ds = [100,100,100,100] \n",
    "    # n = 100\n",
    "    n = 400\n",
    "    sigma = 0.2\n",
    "    alpha = [0.5,0.2,0.2,0.1]\n",
    "    correl = [0,0.1,0.2, 0.5, 0.8]\n",
    "    assert np.round(sum(alpha),2)==1\n",
    "\n",
    "    if run_sim:\n",
    "        R_rho  = synexp(runs = n_runs,sim_type = 'correl',samples_settings=n,ds_settings=ds,y_dim=2,\n",
    "                          correl = correl, alpha_settings = alpha,scale = 0.5,y_noise_settings=sigma)\n",
    "        \n",
    "        R_rho.to_pickle('sweep_rho_cluster_{}.npy'.format(version))\n",
    "    \n",
    "    if plot_sim:\n",
    "        R_rho = pd.read_pickle('sweep_rho_cluster_{}.npy'.format(version))\n",
    "    \n",
    "        var = dict(sigma = sigma, ds = ds, alpha = alpha, n = n)\n",
    "        sim_plots(R_rho,'correl', correl, filename = 'sweep_rho',var_dict = var,ylim0=[-0.1,1],ylim1=[-.2,0.6],\n",
    "                 ylim2=[0,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58dd81b-8f9d-443e-8302-4c6c1bad5f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba0b9e-57f0-47fc-bc8e-cf741a92a2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
